# -*- coding: utf-8 -*-
"""
Created on Thu Jul 23 11:05:00 2020

@author: 한준
"""
# 0723 군집화/실루엣계수
# 군집화 : 데이터가 있는데 임의의 타겟값? 을 지정해줌으로써 묶어주는것

# 몇 만큼 cluster가 되었는가?
# -> 실루엣 계수(s(i)) : 군집화가 얼마나 잘 되어있느냐에 대한 지표.(-1 ~ +1)
# 1. 같은 cluster끼리는 "분산이 작다".
# 2. 서로 다른 cluster끼리는 "분산이 크다".

# a1 = means(a12 a13 a14) 내부 cluster들의 평균값 : 가장 가까이에 있는 "내부" cluster.
# b1 = means(b15 b16 b17 b18) 외부 cluster들의 평균값 : 가장 가까이에 있는 "외부" cluster.

# a1 : 같은 cluster 안에 있는 데이터의 거리의 평균
# b1 : 가장 가까이에 있는 다른 cluster 안에 있는 데이터의 거리의 평균
# 실루엣 계수가 클수록 군집화가 잘 되었다.
# 같은 군집 안에 거리는 좁아지고, B 군집이 멀어지고 -> 최적의 군집화.

# 만약 음수라면 ? 내 군집 안에 다른군집이 들어와있는것.

from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.cm as cm
import math

iris = load_iris()
feature_names = ['sepal_length','sepal_width','petal_length','petal_width']
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)

# 군집화
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300,random_state=0).fit(irisDF)

irisDF['cluster'] = kmeans.labels_

# 실루엣 계수 - 이미 위에서 각각의 데이터를 군집화해놨기 때문에 그 군집에 해당하는 실루엣 계수 구해줌.
score_samples = silhouette_samples(iris.data, irisDF['cluster'])
print('silhouette_samples( ) return 값의 shape' , score_samples.shape)

irisDF['silhouette_coeff'] = score_samples

average_score = silhouette_score(iris.data, irisDF['cluster'])
print('붓꽃 데이터셋 Silhouette Analysis Score:{0:.3f}'.format(average_score))

# 각 cluster별 실루엣 계수 평균.
print(irisDF.groupby('cluster')['silhouette_coeff'].mean())




test1 = []
test2 = []
test3 = []

# K-means의 단점 : 최적의 군집을 직접 설정해줘야한다.
def visualize_silhouette(cluster_lists, X_features):
    global test1
    global test2
    global test3
    n_cols = len(cluster_lists)
    
    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)
    
    # cluster_list 에는 2,3,4,5가 있는데 이것은 k_means의 단점으로서 직접 2,3,4,5 만큼 군집이 있다고 가정한 후에 루프를 돈다.
    # 그래서 군집 2,3,4,5개가 있다고 판단하고 데이터를 군집한다.
    for ind, n_cluster in enumerate(cluster_lists):
   
        # 군집화 한 후에 어떤 군집에 속해있는지 리턴.
        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)
        cluster_labels = clusterer.fit_predict(X_features)
        test1 = cluster_labels
        
        # silhouette_score : 모든 실루엣 계수 평균
        # silhouette_samples : 각각 데이터의 실루엣 계수        
        sil_avg = silhouette_score(X_features, cluster_labels)
        sil_values = silhouette_samples(X_features, cluster_labels)
        
        test2 = sil_values
        
        # 그래프 축 이름 세팅
        y_lower = 10
        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\n' \
                          'Silhouette Score :' + str(round(sil_avg,3)) )
        axs[ind].set_xlabel("The silhouette coefficient values")
        axs[ind].set_ylabel("Cluster label")
        axs[ind].set_xlim([-0.1, 1])
        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])
        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks
        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])

        # 
        for i in range(n_cluster):
            
            # 1번째 루프에서
            # 0~10까지 비어있다 -> 맨 아래 살짝 비는 이유
            # 10~385는 실루엣 계수를 내림차순한 순으로 차있다. 레이블 0으로 군집화 된
            # 385~500은 레이블 1으로 군집화 된 데이터로 차있다.
            # 빨간 선은 평균값.
            # 왜500개지? -> 총 500개를 넣었는데 500개 각각의 실루엣계수가 있기 때문에.
            
            # score : cluster갯수 상관없이 전체 실루엣 계수 평균.
            # 평균 -> 평균에 비슷할수록 제일 좋다. 튀는게 없어야함. -> 군집 4개일 때 제일좋음.
            ith_cluster_sil_values = sil_values[cluster_labels==i]
            ith_cluster_sil_values.sort()
            
            
            size_cluster_i = ith_cluster_sil_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            print(size_cluster_i,i)
            test3.append(size_cluster_i)
            color = cm.nipy_spectral(float(i) / n_cluster)
            
            # y_lower ~ y_upper 까지 채워나가는중
            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \
                                facecolor=color, edgecolor=color, alpha=0.7)
            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            
            # 10번 더한다 -> 중간에 살짝 비는 이유
            y_lower = y_upper + 10
            
        axs[ind].axvline(x=sil_avg, color="red", linestyle="--")

from sklearn.datasets import make_blobs

# 무작위 데이터 클러스터 생성.
X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, \
                  center_box=(-10.0, 10.0), shuffle=True, random_state=1)  

plt.scatter(x=X[:, 0], y=X[: , 1], s=5, color='red',alpha=0.9)

visualize_silhouette([ 2, 3, 4, 5], X)

from sklearn.datasets import load_iris

iris=load_iris()
visualize_silhouette([ 2, 3, 4,5 ], iris.data)

# gmm(가우시안믹스처모델?)
# K-means같은 경우는 점점 중심을 찾아가는 메커니즘이기 때문에 데이터가 원형으로 분포되어있을때는 효과적이다.
# 하지만 데이터 분포가 직사각형식으로 넓게 분포되어있다면? 유동적으로 gmm방식을 써야한다.

# 분산에 루트 -> 표준편차.







